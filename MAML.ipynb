{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74 graphs from /scratch/vsetpal/results/processed_graphs.pkl\n",
      "\n",
      "→ Graph has 6 positives, 300 negatives (window=400)\n",
      "★ Graph 1: sampled 36 total (pos=6, neg=30)\n",
      "\n",
      "→ Graph has 0 positives, 98 negatives (window=400)\n",
      "★ Graph 2: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 4 positives, 2836 negatives (window=400)\n",
      "★ Graph 3: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 0 positives, 1426 negatives (window=400)\n",
      "★ Graph 4: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 3 positives, 1767 negatives (window=400)\n",
      "★ Graph 5: sampled 18 total (pos=3, neg=15)\n",
      "\n",
      "→ Graph has 2 positives, 28 negatives (window=400)\n",
      "★ Graph 6: sampled 12 total (pos=2, neg=10)\n",
      "\n",
      "→ Graph has 0 positives, 4976 negatives (window=400)\n",
      "★ Graph 7: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 34 positives, 1874 negatives (window=400)\n",
      "★ Graph 8: sampled 204 total (pos=34, neg=170)\n",
      "\n",
      "→ Graph has 4 positives, 3890 negatives (window=400)\n",
      "★ Graph 9: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 4 positives, 7280 negatives (window=400)\n",
      "★ Graph 10: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 14 positives, 2678 negatives (window=400)\n",
      "★ Graph 11: sampled 84 total (pos=14, neg=70)\n",
      "\n",
      "→ Graph has 4 positives, 1302 negatives (window=400)\n",
      "★ Graph 12: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 0 positives, 380 negatives (window=400)\n",
      "★ Graph 13: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 0 positives, 622 negatives (window=400)\n",
      "★ Graph 14: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 6 positives, 6834 negatives (window=400)\n",
      "★ Graph 15: sampled 36 total (pos=6, neg=30)\n",
      "\n",
      "→ Graph has 1 positives, 991 negatives (window=400)\n",
      "★ Graph 16: sampled 6 total (pos=1, neg=5)\n",
      "\n",
      "→ Graph has 4 positives, 1976 negatives (window=400)\n",
      "★ Graph 17: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 1 positives, 1559 negatives (window=400)\n",
      "★ Graph 18: sampled 6 total (pos=1, neg=5)\n",
      "\n",
      "→ Graph has 4 positives, 548 negatives (window=400)\n",
      "★ Graph 19: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 3 positives, 13675 negatives (window=400)\n",
      "★ Graph 20: sampled 18 total (pos=3, neg=15)\n",
      "\n",
      "→ Graph has 3 positives, 1445 negatives (window=400)\n",
      "★ Graph 21: sampled 18 total (pos=3, neg=15)\n",
      "\n",
      "→ Graph has 0 positives, 462 negatives (window=400)\n",
      "★ Graph 22: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 0 positives, 156 negatives (window=400)\n",
      "★ Graph 23: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 2 positives, 6782 negatives (window=400)\n",
      "★ Graph 24: sampled 12 total (pos=2, neg=10)\n",
      "\n",
      "→ Graph has 27 positives, 765 negatives (window=400)\n",
      "★ Graph 25: sampled 162 total (pos=27, neg=135)\n",
      "\n",
      "→ Graph has 2 positives, 3926 negatives (window=400)\n",
      "★ Graph 26: sampled 12 total (pos=2, neg=10)\n",
      "\n",
      "→ Graph has 6 positives, 986 negatives (window=400)\n",
      "★ Graph 27: sampled 36 total (pos=6, neg=30)\n",
      "\n",
      "→ Graph has 7 positives, 1791 negatives (window=400)\n",
      "★ Graph 28: sampled 42 total (pos=7, neg=35)\n",
      "\n",
      "→ Graph has 7 positives, 4255 negatives (window=400)\n",
      "★ Graph 29: sampled 42 total (pos=7, neg=35)\n",
      "\n",
      "→ Graph has 4 positives, 2136 negatives (window=400)\n",
      "★ Graph 30: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 18 positives, 2402 negatives (window=400)\n",
      "★ Graph 31: sampled 108 total (pos=18, neg=90)\n",
      "\n",
      "→ Graph has 55 positives, 6829 negatives (window=400)\n",
      "★ Graph 32: sampled 330 total (pos=55, neg=275)\n",
      "\n",
      "→ Graph has 14 positives, 2980 negatives (window=400)\n",
      "★ Graph 33: sampled 84 total (pos=14, neg=70)\n",
      "\n",
      "→ Graph has 52 positives, 12892 negatives (window=400)\n",
      "★ Graph 34: sampled 312 total (pos=52, neg=260)\n",
      "\n",
      "→ Graph has 4 positives, 2224 negatives (window=400)\n",
      "★ Graph 35: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 11 positives, 2043 negatives (window=400)\n",
      "★ Graph 36: sampled 66 total (pos=11, neg=55)\n",
      "\n",
      "→ Graph has 42 positives, 1470 negatives (window=400)\n",
      "★ Graph 37: sampled 252 total (pos=42, neg=210)\n",
      "\n",
      "→ Graph has 17 positives, 2053 negatives (window=400)\n",
      "★ Graph 38: sampled 102 total (pos=17, neg=85)\n",
      "\n",
      "→ Graph has 16 positives, 854 negatives (window=400)\n",
      "★ Graph 39: sampled 96 total (pos=16, neg=80)\n",
      "\n",
      "→ Graph has 17 positives, 5127 negatives (window=400)\n",
      "★ Graph 40: sampled 102 total (pos=17, neg=85)\n",
      "\n",
      "→ Graph has 31 positives, 11741 negatives (window=400)\n",
      "★ Graph 41: sampled 186 total (pos=31, neg=155)\n",
      "\n",
      "→ Graph has 11 positives, 1637 negatives (window=400)\n",
      "★ Graph 42: sampled 66 total (pos=11, neg=55)\n",
      "\n",
      "→ Graph has 26 positives, 43664 negatives (window=400)\n",
      "★ Graph 43: sampled 156 total (pos=26, neg=130)\n",
      "\n",
      "→ Graph has 35 positives, 25957 negatives (window=400)\n",
      "★ Graph 44: sampled 210 total (pos=35, neg=175)\n",
      "\n",
      "→ Graph has 9 positives, 11577 negatives (window=400)\n",
      "★ Graph 45: sampled 54 total (pos=9, neg=45)\n",
      "\n",
      "→ Graph has 17 positives, 1105 negatives (window=400)\n",
      "★ Graph 46: sampled 102 total (pos=17, neg=85)\n",
      "\n",
      "→ Graph has 14 positives, 8026 negatives (window=400)\n",
      "★ Graph 47: sampled 84 total (pos=14, neg=70)\n",
      "\n",
      "→ Graph has 40 positives, 9802 negatives (window=400)\n",
      "★ Graph 48: sampled 240 total (pos=40, neg=200)\n",
      "\n",
      "→ Graph has 19 positives, 9615 negatives (window=400)\n",
      "★ Graph 49: sampled 114 total (pos=19, neg=95)\n",
      "\n",
      "→ Graph has 0 positives, 4060 negatives (window=400)\n",
      "★ Graph 50: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 18 positives, 14056 negatives (window=400)\n",
      "★ Graph 51: sampled 108 total (pos=18, neg=90)\n",
      "\n",
      "→ Graph has 30 positives, 1200 negatives (window=400)\n",
      "★ Graph 52: sampled 180 total (pos=30, neg=150)\n",
      "\n",
      "→ Graph has 10 positives, 2538 negatives (window=400)\n",
      "★ Graph 53: sampled 60 total (pos=10, neg=50)\n",
      "\n",
      "→ Graph has 18 positives, 2410 negatives (window=400)\n",
      "★ Graph 54: sampled 108 total (pos=18, neg=90)\n",
      "\n",
      "→ Graph has 4 positives, 178 negatives (window=400)\n",
      "★ Graph 55: sampled 24 total (pos=4, neg=20)\n",
      "\n",
      "→ Graph has 7 positives, 1873 negatives (window=400)\n",
      "★ Graph 56: sampled 42 total (pos=7, neg=35)\n",
      "\n",
      "→ Graph has 40 positives, 1114 negatives (window=400)\n",
      "★ Graph 57: sampled 240 total (pos=40, neg=200)\n",
      "\n",
      "→ Graph has 18 positives, 3676 negatives (window=400)\n",
      "★ Graph 58: sampled 108 total (pos=18, neg=90)\n",
      "\n",
      "→ Graph has 8 positives, 2216 negatives (window=400)\n",
      "★ Graph 59: sampled 48 total (pos=8, neg=40)\n",
      "\n",
      "→ Graph has 3 positives, 5485 negatives (window=400)\n",
      "★ Graph 60: sampled 18 total (pos=3, neg=15)\n",
      "\n",
      "→ Graph has 13 positives, 1565 negatives (window=400)\n",
      "★ Graph 61: sampled 78 total (pos=13, neg=65)\n",
      "\n",
      "→ Graph has 12 positives, 3744 negatives (window=400)\n",
      "★ Graph 62: sampled 72 total (pos=12, neg=60)\n",
      "\n",
      "→ Graph has 34 positives, 3800 negatives (window=400)\n",
      "★ Graph 63: sampled 204 total (pos=34, neg=170)\n",
      "\n",
      "→ Graph has 20 positives, 3162 negatives (window=400)\n",
      "★ Graph 64: sampled 120 total (pos=20, neg=100)\n",
      "\n",
      "→ Graph has 7 positives, 3587 negatives (window=400)\n",
      "★ Graph 65: sampled 42 total (pos=7, neg=35)\n",
      "\n",
      "→ Graph has 12 positives, 744 negatives (window=400)\n",
      "★ Graph 66: sampled 72 total (pos=12, neg=60)\n",
      "\n",
      "→ Graph has 3 positives, 179 negatives (window=400)\n",
      "★ Graph 67: sampled 18 total (pos=3, neg=15)\n",
      "\n",
      "→ Graph has 22 positives, 874 negatives (window=400)\n",
      "★ Graph 68: sampled 132 total (pos=22, neg=110)\n",
      "\n",
      "→ Graph has 3 positives, 1329 negatives (window=400)\n",
      "★ Graph 69: sampled 18 total (pos=3, neg=15)\n",
      "\n",
      "→ Graph has 2 positives, 3028 negatives (window=400)\n",
      "★ Graph 70: sampled 12 total (pos=2, neg=10)\n",
      "\n",
      "→ Graph has 0 positives, 20 negatives (window=400)\n",
      "★ Graph 71: sampled 0 total (pos=0, neg=0)\n",
      "\n",
      "→ Graph has 45 positives, 5009 negatives (window=400)\n",
      "★ Graph 72: sampled 270 total (pos=45, neg=225)\n",
      "\n",
      "→ Graph has 4 positives, 16 negatives (window=400)\n",
      "★ Graph 73: sampled 20 total (pos=4, neg=16)\n",
      "\n",
      "→ Graph has 17 positives, 1703 negatives (window=400)\n",
      "★ Graph 74: sampled 102 total (pos=17, neg=85)\n",
      "\n",
      "Total training pairs across all graphs: 5696\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "# Load your already‐processed graphs\n",
    "PICKLE_PATH = \"processed_graphs.pkl\" \n",
    "with open(PICKLE_PATH, \"rb\") as f:\n",
    "    all_graphs = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_graphs)} graphs from {PICKLE_PATH}\\n\")\n",
    "\n",
    "# Define the pair‐generation & balancing functions\n",
    "def generate_entity_pairs(graph, max_distance=400):\n",
    "    entities = graph[\"entities\"]\n",
    "    # build a quick lookup of gold relations\n",
    "    gold = { (r[\"head\"], r[\"tail\"]) : r[\"label\"]\n",
    "             for r in graph.get(\"relation_labels\", []) }\n",
    "\n",
    "    pos_pairs = []\n",
    "    neg_pairs = []\n",
    "    for h in entities:\n",
    "        for t in entities:\n",
    "            if h is t:\n",
    "                continue\n",
    "            dh = abs(h[\"root\"] - t[\"root\"])\n",
    "            if dh > max_distance:\n",
    "                continue\n",
    "            lbl = gold.get((h[\"root\"], t[\"root\"]), \"NoRelation\")\n",
    "            entry = {\n",
    "                \"head\":       h,\n",
    "                \"tail\":       t,\n",
    "                \"head_idx\":   h[\"root\"],\n",
    "                \"tail_idx\":   t[\"root\"],\n",
    "                \"label\":      lbl\n",
    "            }\n",
    "            if lbl == \"NoRelation\":\n",
    "                neg_pairs.append(entry)\n",
    "            else:\n",
    "                pos_pairs.append(entry)\n",
    "\n",
    "    print(f\"→ Graph has {len(pos_pairs)} positives, {len(neg_pairs)} negatives (window={max_distance})\")\n",
    "    return pos_pairs, neg_pairs\n",
    "\n",
    "def balance_pairs(pos_pairs, neg_pairs, neg_pos_ratio=5):\n",
    "    random.shuffle(neg_pairs)\n",
    "    k = min(len(neg_pairs), len(pos_pairs) * neg_pos_ratio)\n",
    "    sampled_neg = neg_pairs[:k]\n",
    "    return pos_pairs + sampled_neg\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Iterate over all graphs to build a per‐graph training set\n",
    "all_train_pairs = []\n",
    "for idx, graph in enumerate(all_graphs, start=1):\n",
    "    pos, neg = generate_entity_pairs(graph,\n",
    "                                     max_distance=400)\n",
    "    train_pairs = balance_pairs(pos, neg,\n",
    "                                neg_pos_ratio=5)\n",
    "    random.shuffle(train_pairs)\n",
    "    all_train_pairs.append(train_pairs)\n",
    "    print(f\"★ Graph {idx}: sampled {len(train_pairs)} total (pos={len(pos)}, neg={len(train_pairs)-len(pos)})\\n\")\n",
    "\n",
    "# Flatten across all graphs\n",
    "flat_pairs = [pair for gp in all_train_pairs for pair in gp]\n",
    "print(f\"Total training pairs across all graphs: {len(flat_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d155fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74 graphs\n",
      "\n",
      "\n",
      "Graph 1/74: 18 entities\n",
      "  • sampled 12 pairs (pos=6, neg=6)\n",
      "\n",
      "Graph 2/74: 14 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 3/74: 59 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 4/74: 60 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 5/74: 54 entities\n",
      "  • sampled 6 pairs (pos=3, neg=3)\n",
      "\n",
      "Graph 6/74: 6 entities\n",
      "  • sampled 4 pairs (pos=2, neg=2)\n",
      "\n",
      "Graph 7/74: 123 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 8/74: 68 entities\n",
      "  • sampled 68 pairs (pos=34, neg=34)\n",
      "\n",
      "Graph 9/74: 82 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 10/74: 99 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 11/74: 59 entities\n",
      "  • sampled 28 pairs (pos=14, neg=14)\n",
      "\n",
      "Graph 12/74: 46 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 13/74: 20 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 14/74: 42 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 15/74: 103 entities\n",
      "  • sampled 12 pairs (pos=6, neg=6)\n",
      "\n",
      "Graph 16/74: 32 entities\n",
      "  • sampled 2 pairs (pos=1, neg=1)\n",
      "\n",
      "Graph 17/74: 45 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 18/74: 40 entities\n",
      "  • sampled 2 pairs (pos=1, neg=1)\n",
      "\n",
      "Graph 19/74: 24 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 20/74: 143 entities\n",
      "  • sampled 6 pairs (pos=3, neg=3)\n",
      "\n",
      "Graph 21/74: 50 entities\n",
      "  • sampled 6 pairs (pos=3, neg=3)\n",
      "\n",
      "Graph 22/74: 22 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 23/74: 13 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 24/74: 149 entities\n",
      "  • sampled 4 pairs (pos=2, neg=2)\n",
      "\n",
      "Graph 25/74: 32 entities\n",
      "  • sampled 54 pairs (pos=27, neg=27)\n",
      "\n",
      "Graph 26/74: 83 entities\n",
      "  • sampled 4 pairs (pos=2, neg=2)\n",
      "\n",
      "Graph 27/74: 32 entities\n",
      "  • sampled 12 pairs (pos=6, neg=6)\n",
      "\n",
      "Graph 28/74: 47 entities\n",
      "  • sampled 14 pairs (pos=7, neg=7)\n",
      "\n",
      "Graph 29/74: 101 entities\n",
      "  • sampled 14 pairs (pos=7, neg=7)\n",
      "\n",
      "Graph 30/74: 47 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 31/74: 56 entities\n",
      "  • sampled 36 pairs (pos=18, neg=18)\n",
      "\n",
      "Graph 32/74: 121 entities\n",
      "  • sampled 110 pairs (pos=55, neg=55)\n",
      "\n",
      "Graph 33/74: 60 entities\n",
      "  • sampled 28 pairs (pos=14, neg=14)\n",
      "\n",
      "Graph 34/74: 195 entities\n",
      "  • sampled 104 pairs (pos=52, neg=52)\n",
      "\n",
      "Graph 35/74: 52 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 36/74: 63 entities\n",
      "  • sampled 22 pairs (pos=11, neg=11)\n",
      "\n",
      "Graph 37/74: 49 entities\n",
      "  • sampled 84 pairs (pos=42, neg=42)\n",
      "\n",
      "Graph 38/74: 46 entities\n",
      "  • sampled 34 pairs (pos=17, neg=17)\n",
      "\n",
      "Graph 39/74: 30 entities\n",
      "  • sampled 32 pairs (pos=16, neg=16)\n",
      "\n",
      "Graph 40/74: 94 entities\n",
      "  • sampled 34 pairs (pos=17, neg=17)\n",
      "\n",
      "Graph 41/74: 189 entities\n",
      "  • sampled 62 pairs (pos=31, neg=31)\n",
      "\n",
      "Graph 42/74: 44 entities\n",
      "  • sampled 22 pairs (pos=11, neg=11)\n",
      "\n",
      "Graph 43/74: 412 entities\n",
      "  • sampled 52 pairs (pos=26, neg=26)\n",
      "\n",
      "Graph 44/74: 251 entities\n",
      "  • sampled 70 pairs (pos=35, neg=35)\n",
      "\n",
      "Graph 45/74: 159 entities\n",
      "  • sampled 18 pairs (pos=9, neg=9)\n",
      "\n",
      "Graph 46/74: 34 entities\n",
      "  • sampled 34 pairs (pos=17, neg=17)\n",
      "\n",
      "Graph 47/74: 106 entities\n",
      "  • sampled 28 pairs (pos=14, neg=14)\n",
      "\n",
      "Graph 48/74: 128 entities\n",
      "  • sampled 80 pairs (pos=40, neg=40)\n",
      "\n",
      "Graph 49/74: 135 entities\n",
      "  • sampled 38 pairs (pos=19, neg=19)\n",
      "\n",
      "Graph 50/74: 79 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 51/74: 165 entities\n",
      "  • sampled 36 pairs (pos=18, neg=18)\n",
      "\n",
      "Graph 52/74: 60 entities\n",
      "  • sampled 60 pairs (pos=30, neg=30)\n",
      "\n",
      "Graph 53/74: 83 entities\n",
      "  • sampled 20 pairs (pos=10, neg=10)\n",
      "\n",
      "Graph 54/74: 70 entities\n",
      "  • sampled 36 pairs (pos=18, neg=18)\n",
      "\n",
      "Graph 55/74: 14 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 56/74: 67 entities\n",
      "  • sampled 14 pairs (pos=7, neg=7)\n",
      "\n",
      "Graph 57/74: 51 entities\n",
      "  • sampled 80 pairs (pos=40, neg=40)\n",
      "\n",
      "Graph 58/74: 87 entities\n",
      "  • sampled 36 pairs (pos=18, neg=18)\n",
      "\n",
      "Graph 59/74: 72 entities\n",
      "  • sampled 16 pairs (pos=8, neg=8)\n",
      "\n",
      "Graph 60/74: 101 entities\n",
      "  • sampled 6 pairs (pos=3, neg=3)\n",
      "\n",
      "Graph 61/74: 48 entities\n",
      "  • sampled 26 pairs (pos=13, neg=13)\n",
      "\n",
      "Graph 62/74: 107 entities\n",
      "  • sampled 24 pairs (pos=12, neg=12)\n",
      "\n",
      "Graph 63/74: 91 entities\n",
      "  • sampled 68 pairs (pos=34, neg=34)\n",
      "\n",
      "Graph 64/74: 91 entities\n",
      "  • sampled 40 pairs (pos=20, neg=20)\n",
      "\n",
      "Graph 65/74: 71 entities\n",
      "  • sampled 14 pairs (pos=7, neg=7)\n",
      "\n",
      "Graph 66/74: 28 entities\n",
      "  • sampled 24 pairs (pos=12, neg=12)\n",
      "\n",
      "Graph 67/74: 14 entities\n",
      "  • sampled 6 pairs (pos=3, neg=3)\n",
      "\n",
      "Graph 68/74: 42 entities\n",
      "  • sampled 44 pairs (pos=22, neg=22)\n",
      "\n",
      "Graph 69/74: 37 entities\n",
      "  • sampled 6 pairs (pos=3, neg=3)\n",
      "\n",
      "Graph 70/74: 60 entities\n",
      "  • sampled 4 pairs (pos=2, neg=2)\n",
      "\n",
      "Graph 71/74: 5 entities\n",
      "  • sampled 0 pairs (pos=0, neg=0)\n",
      "\n",
      "Graph 72/74: 74 entities\n",
      "  • sampled 90 pairs (pos=45, neg=45)\n",
      "\n",
      "Graph 73/74: 5 entities\n",
      "  • sampled 8 pairs (pos=4, neg=4)\n",
      "\n",
      "Graph 74/74: 44 entities\n",
      "  • sampled 34 pairs (pos=17, neg=17)\n",
      "\n",
      "Feature matrix shape: (1900, 1536)\n",
      "Labels vector shape:  (1900,)\n",
      "Relation label → ID mapping:\n",
      "     0: NoRelation\n",
      "     1: Creator\n",
      "     2: LocatedIn\n",
      "     3: UsedBy\n",
      "     4: PartOf\n",
      "     5: OwnerOf\n",
      "     6: Affiliation\n",
      "     7: AppliesToPeople\n",
      "     8: HasPart\n",
      "     9: InfluencedBy\n",
      "    10: HasWorksInTheCollection\n",
      "    11: Uses\n",
      "    12: Country\n",
      "    13: DiplomaticRelation\n",
      "    14: Continent\n",
      "    15: OfficialLanguage\n",
      "    16: PresentedIn\n",
      "    17: DifferentFrom\n",
      "    18: Studies\n",
      "    19: BasedOn\n",
      "    20: RegulatedBy\n",
      "    21: NamedAfter\n",
      "    22: WorkLocation\n",
      "    23: InterestedIn\n",
      "    24: AcademicDegree\n",
      "    25: CountryOfCitizenship\n",
      "    26: Author\n",
      "    27: InOppositionTo\n",
      "    28: EducatedAt\n",
      "    29: OwnedBy\n",
      "    30: Partner\n",
      "    31: SaidToBeTheSameAs\n",
      "    32: SignificantEvent\n",
      "    33: PhysicallyInteractsWith\n",
      "    34: HasEffect\n",
      "    35: Founded\n",
      "    36: Follows\n",
      "    37: InspiredBy\n",
      "    38: NominatedFor\n",
      "    39: PracticedBy\n",
      "    40: PrimeFactor\n",
      "    41: ContainsTheAdministrativeTerritorialEntity\n",
      "    42: ParentOrganization\n",
      "    43: FollowedBy\n",
      "    44: NativeLanguage\n",
      "    45: OriginalLanguageOfFilmOrTvShow\n",
      "    46: Director\n",
      "    47: PositionHeld\n",
      "    48: PlaceOfBirth\n",
      "    49: OperatingSystem\n",
      "    50: IssuedBy\n",
      "    51: CitesWork\n",
      "    52: PublishedIn\n",
      "    53: Employer\n",
      "    54: SharesBorderWith\n",
      "    55: Developer\n",
      "    56: AdjacentStation\n",
      "    57: ContributedToCreativeWork\n",
      "    58: FieldOfWork\n",
      "    59: ApprovedBy\n",
      "    60: Causes\n",
      "    61: HasCause\n",
      "    62: NamedBy\n",
      "    63: FoundedBy\n",
      "    64: AwardReceived\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "#Load processed graphs\n",
    "with open(\"processed_graphs.pkl\", \"rb\") as f:\n",
    "    all_graphs = pickle.load(f)\n",
    "print(f\"Loaded {len(all_graphs)} graphs\\n\")\n",
    "\n",
    "# Sampling utilities\n",
    "def generate_entity_pairs(graph, max_distance=400):\n",
    "    entities = graph[\"entities\"]\n",
    "    gold = {(r[\"head\"], r[\"tail\"]): r[\"label\"]\n",
    "            for r in graph.get(\"relation_labels\", [])}\n",
    "\n",
    "    pos_pairs, neg_pairs = [], []\n",
    "    for h in entities:\n",
    "        for t in entities:\n",
    "            if h is t: continue\n",
    "            if abs(h[\"root\"] - t[\"root\"]) > max_distance:\n",
    "                continue\n",
    "            label = gold.get((h[\"root\"], t[\"root\"]), \"NoRelation\")\n",
    "            entry = {\n",
    "                \"head\": h,\n",
    "                \"tail\": t,\n",
    "                \"head_idx\": h[\"root\"],\n",
    "                \"tail_idx\": t[\"root\"],\n",
    "                \"label\": label\n",
    "            }\n",
    "            if label == \"NoRelation\":\n",
    "                neg_pairs.append(entry)\n",
    "            else:\n",
    "                pos_pairs.append(entry)\n",
    "    return pos_pairs, neg_pairs\n",
    "\n",
    "def balance_pairs_1to1(pos_pairs, neg_pairs):\n",
    "    k = min(len(pos_pairs), len(neg_pairs))\n",
    "    neg_sample = random.sample(neg_pairs, k)\n",
    "    return pos_pairs + neg_sample\n",
    "\n",
    "# Helper to average span embeddings\n",
    "def average_span_embedding(bert_tensor, start, end):\n",
    "    if start >= end or end > bert_tensor.size(0):\n",
    "        return torch.zeros(bert_tensor.size(1), device=bert_tensor.device)\n",
    "    return bert_tensor[start:end].mean(dim=0)\n",
    "\n",
    "# Build relation‑level dataset\n",
    "def extract_relation_dataset_1to1(graphs, max_distance=400, device=None, debug=False):\n",
    "    X, y = [], []\n",
    "    relation_label_map = defaultdict(lambda: len(relation_label_map))\n",
    "    relation_label_map[\"NoRelation\"] = 0\n",
    "\n",
    "    for gi, graph in enumerate(graphs, 1):\n",
    "        if debug:\n",
    "            print(f\"\\nGraph {gi}/{len(graphs)}: {len(graph['entities'])} entities\")\n",
    "\n",
    "        bert_tensor = torch.tensor(\n",
    "            graph[\"node_features\"], dtype=torch.float, device=device\n",
    "        )\n",
    "\n",
    "        pos, neg = generate_entity_pairs(graph, max_distance)\n",
    "        pairs = balance_pairs_1to1(pos, neg)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"  • sampled {len(pairs)} pairs (pos={len(pos)}, neg={len(pairs)-len(pos)})\")\n",
    "\n",
    "        for entry in pairs:\n",
    "            h, t = entry[\"head\"], entry[\"tail\"]\n",
    "            lbl = entry[\"label\"]\n",
    "            lbl_id = relation_label_map[lbl]\n",
    "\n",
    "            head_vec = average_span_embedding(bert_tensor, h[\"start\"], h[\"end\"])\n",
    "            tail_vec = average_span_embedding(bert_tensor, t[\"start\"], t[\"end\"])\n",
    "            pair_vec = torch.cat([head_vec, tail_vec], dim=0).cpu().numpy()\n",
    "\n",
    "            X.append(pair_vec)\n",
    "            y.append(lbl_id)\n",
    "\n",
    "    return np.stack(X), np.array(y), dict(relation_label_map)\n",
    "\n",
    "# Run and inspect\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X, y, label_map = extract_relation_dataset_1to1(\n",
    "    all_graphs,\n",
    "    max_distance=400,\n",
    "    device=device,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Labels vector shape:  {y.shape}\")\n",
    "print(\"Relation label → ID mapping:\")\n",
    "for lbl, idx in sorted(label_map.items(), key=lambda x: x[1]):\n",
    "    print(f\"   {idx:>3}: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2211c18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded NER label map from checkpoint.\n",
      "Loaded 74 training graphs\n",
      "Finished 1:1 positive∶negative balancing on each graph.\n",
      "Rebuilt RE label map: {'NoRelation': 0, 'AcademicDegree': 1, 'AdjacentStation': 2, 'Affiliation': 3, 'AppliesToPeople': 4, 'ApprovedBy': 5, 'Author': 6, 'AwardReceived': 7, 'BasedOn': 8, 'Capital': 9, 'Causes': 10, 'CitesWork': 11, 'ContainsAdministrativeTerritorialEntity': 12, 'ContainsTheAdministrativeTerritorialEntity': 13, 'Continent': 14, 'ContributedToCreativeWork': 15, 'Country': 16, 'CountryOfCitizenship': 17, 'Creator': 18, 'Developer': 19, 'DifferentFrom': 20, 'DiplomaticRelation': 21, 'Director': 22, 'EducatedAt': 23, 'Employer': 24, 'FieldOfWork': 25, 'FollowedBy': 26, 'Follows': 27, 'Founded': 28, 'FoundedBy': 29, 'HasCause': 30, 'HasEffect': 31, 'HasPart': 32, 'HasQuality': 33, 'HasWorksInTheCollection': 34, 'InOppositionTo': 35, 'InfluencedBy': 36, 'InspiredBy': 37, 'InterestedIn': 38, 'IssuedBy': 39, 'LanguageOfWorkOrName': 40, 'LanguageUsed': 41, 'LocatedIn': 42, 'Location': 43, 'MemberOf': 44, 'NamedAfter': 45, 'NamedBy': 46, 'NativeLanguage': 47, 'NominatedFor': 48, 'OfficialLanguage': 49, 'OperatingSystem': 50, 'OriginalLanguageOfFilmOrTvShow': 51, 'OwnedBy': 52, 'OwnerOf': 53, 'ParentOrganization': 54, 'PartOf': 55, 'Partner': 56, 'PartyChiefRepresentative': 57, 'PhysicallyInteractsWith': 58, 'PlaceOfBirth': 59, 'PositionHeld': 60, 'PracticedBy': 61, 'PresentedIn': 62, 'PrimeFactor': 63, 'Promoted': 64, 'PublishedIn': 65, 'RegulatedBy': 66, 'SaidToBeTheSameAs': 67, 'SharesBorderWith': 68, 'SignificantEvent': 69, 'Studies': 70, 'TwinnedAdministrativeBody': 71, 'UsedBy': 72, 'Uses': 73, 'WorkLocation': 74}\n",
      "Meta‐training GNN_RE_NER\n",
      "Meta‐Epoch 1/50\n",
      "Avg Query Loss: 7.1420\n",
      "Meta‐Epoch 2/50\n",
      "Avg Query Loss: 7.1420\n",
      "Meta‐Epoch 3/50\n",
      "Avg Query Loss: 7.1420\n",
      "Meta‐Epoch 4/50\n",
      "Avg Query Loss: 7.1420\n",
      "Meta‐Epoch 5/50\n",
      "Avg Query Loss: 7.1420\n",
      "Meta‐Epoch 6/50\n",
      "Avg Query Loss: 7.1420\n",
      "Early stopping\n",
      "Saved meta‐trained checkpoint to /scratch/vsetpal/results/trained_gnn_checkpoint_meta.pt\n",
      "Annotating all test documents\n",
      "Evaluation across domains\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tabulate import tabulate\n",
    "import higher\n",
    "\n",
    "# 1. Setup & Model Loading\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "nlp       = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert      = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "bert.eval()\n",
    "\n",
    "# 2. GNN Model\n",
    "class GNN_RE_NER(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_ner_classes, num_re_classes):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(input_dim,  hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.ner_classifier = nn.Linear(hidden_dim, num_ner_classes)\n",
    "        self.re_classifier  = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_re_classes)\n",
    "        )\n",
    "    def forward(self, x, edge_index, entity_pairs):\n",
    "        x = self.gcn1(x, edge_index).relu()\n",
    "        x = self.gcn2(x, edge_index).relu()\n",
    "        ner_logits = self.ner_classifier(x)\n",
    "        pair_repr = []\n",
    "        for h,t in entity_pairs:\n",
    "            pair_repr.append(torch.cat([x[h], x[t]], dim=-1))\n",
    "        if pair_repr:\n",
    "            re_input = torch.stack(pair_repr)\n",
    "            re_logits = self.re_classifier(re_input)\n",
    "        else:\n",
    "            re_logits = torch.empty(0, self.re_classifier[-1].out_features, device=x.device)\n",
    "        return ner_logits, re_logits\n",
    "\n",
    "# 3. Utility Functions\n",
    "@torch.no_grad()\n",
    "def get_bert_embeddings(text):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True,\n",
    "                    truncation=True, max_length=512).to(device)\n",
    "    offsets = enc.pop(\"offset_mapping\")[0].tolist()\n",
    "    out = bert(**enc)\n",
    "    return tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0]), out.last_hidden_state.squeeze(0), offsets\n",
    "\n",
    "def align_to_bert(doc, tokens, embeddings, offsets):\n",
    "    feats = []\n",
    "    for tok in doc:\n",
    "        s,e = tok.idx, tok.idx+len(tok)\n",
    "        matched = [embeddings[i] \n",
    "                   for i,(st,ed) in enumerate(offsets) \n",
    "                   if st>=s and ed<=e]\n",
    "        if matched:\n",
    "            feats.append(torch.mean(torch.stack(matched),0).cpu().tolist())\n",
    "        else:\n",
    "            feats.append([0.]*embeddings.size(-1))\n",
    "    return feats\n",
    "\n",
    "def group_tokens_into_entities(token_texts, token_labels, id2label):\n",
    "    ents, i = [], 0\n",
    "    while i < len(token_texts):\n",
    "        lab = id2label[token_labels[i]]\n",
    "        if lab!=\"O\":\n",
    "            start = i\n",
    "            toks  = [token_texts[i]]\n",
    "            while i+1<len(token_texts) and id2label[token_labels[i+1]]==lab:\n",
    "                i+=1; toks.append(token_texts[i])\n",
    "            ents.append({\n",
    "                \"text\":\" \".join(toks),\n",
    "                \"label\":lab,\n",
    "                \"start\":start,\"end\":i+1,\"root\":start\n",
    "            })\n",
    "        i+=1\n",
    "    return ents\n",
    "\n",
    "# 4. Load NER Label Map\n",
    "checkpoint_path = \"Your_Checkpoint_Path/checkpoint.pth\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    ner_label_map = ckpt[\"ner_label_map\"]\n",
    "    print(\"Loaded NER label map from checkpoint.\")\n",
    "else:\n",
    "    ner_label_map = {\"O\":0, \"PERSON\":1, \"ORG\":2, \"GPE\":3, \"DATE\":4}\n",
    "    print(\"Defined NER label map manually.\")\n",
    "\n",
    "id2ner = {v:k for k,v in ner_label_map.items()}\n",
    "\n",
    "# 5. Load & Balance Processed Training Graphs (1:1 positive∶negative)\n",
    "graphs_pkl = \"/scratch/vsetpal/results/processed_graphs.pkl\"\n",
    "with open(graphs_pkl,\"rb\") as f:\n",
    "    training_graphs = pickle.load(f)\n",
    "print(f\"Loaded {len(training_graphs)} training graphs\")\n",
    "\n",
    "def augment_with_negatives(graph, max_distance=400):\n",
    "    positives = list(graph.get(\"relation_labels\", []))\n",
    "    gold = {(r[\"head\"], r[\"tail\"]) for r in positives}\n",
    "    neg_cands = []\n",
    "    for e1 in graph[\"entities\"]:\n",
    "        for e2 in graph[\"entities\"]:\n",
    "            if e1[\"root\"]==e2[\"root\"]: continue\n",
    "            if abs(e1[\"root\"]-e2[\"root\"])>max_distance: continue\n",
    "            if (e1[\"root\"],e2[\"root\"]) in gold: continue\n",
    "            neg_cands.append({\"head\":e1[\"root\"], \"tail\":e2[\"root\"], \"label\":\"NoRelation\"})\n",
    "    k = min(len(neg_cands), len(positives))\n",
    "    sampled = random.sample(neg_cands, k) if k>0 else []\n",
    "    graph[\"relation_labels\"] = positives + sampled\n",
    "\n",
    "for g in training_graphs:\n",
    "    augment_with_negatives(g, max_distance=400)\n",
    "print(\"Finished 1:1 positive∶negative balancing on each graph.\")\n",
    "\n",
    "# 5b. Rebuild RE label map from balanced graphs\n",
    "all_re_labels = {\"NoRelation\"}\n",
    "for g in training_graphs:\n",
    "    for r in g[\"relation_labels\"]:\n",
    "        all_re_labels.add(r[\"label\"])\n",
    "labels_sorted = [\"NoRelation\"] + sorted(l for l in all_re_labels if l!=\"NoRelation\")\n",
    "relation_label_map = {lbl:i for i,lbl in enumerate(labels_sorted)}\n",
    "id2rel = {i:lbl for lbl,i in relation_label_map.items()}\n",
    "print(\"Rebuilt RE label map:\", relation_label_map)\n",
    "\n",
    "# 6. Meta‐Training (MAML)\n",
    "def split_graph(g):\n",
    "    rels = g.get(\"relation_labels\",[])\n",
    "    m = len(rels)//2\n",
    "    sg, qg = dict(g), dict(g)\n",
    "    sg[\"relation_labels\"], qg[\"relation_labels\"] = rels[:m], rels[m:]\n",
    "    return sg,qg\n",
    "\n",
    "def compute_loss(model, entity_embedder, g):\n",
    "    nf = torch.tensor(g[\"node_features\"],dtype=torch.float,device=device)\n",
    "    edge_idx = torch.tensor(g[\"edges\"],dtype=torch.long).t().to(device)\n",
    "    ner_seq = [\"O\"]*len(g[\"nodes\"])\n",
    "    for ent in g.get(\"entities\",[]):\n",
    "        for i in range(ent[\"start\"],ent[\"end\"]):\n",
    "            if i<len(ner_seq): ner_seq[i] = ent[\"label\"]\n",
    "    ner_ids = torch.tensor([ner_label_map.get(t,0) for t in ner_seq],\n",
    "                           dtype=torch.long,device=device)\n",
    "    ner_emb = entity_embedder(ner_ids)\n",
    "    X = torch.cat([nf, ner_emb], dim=-1)\n",
    "\n",
    "    pairs = [(r[\"head\"],r[\"tail\"]) for r in g.get(\"relation_labels\",[])]\n",
    "    ner_logits, re_logits = model(X, edge_idx, pairs)\n",
    "\n",
    "    l_ner = F.cross_entropy(ner_logits, ner_ids)\n",
    "    if re_logits.size(0)>0:\n",
    "        lbls = torch.tensor([relation_label_map[r[\"label\"]] \n",
    "                             for r in g[\"relation_labels\"]],\n",
    "                            dtype=torch.long,device=device)\n",
    "        l_re = F.cross_entropy(re_logits, lbls)\n",
    "    else:\n",
    "        l_re = torch.tensor(0., device=device)\n",
    "    return l_ner + l_re\n",
    "\n",
    "def meta_train(graphs, ner_label_map, relation_label_map,\n",
    "               entity_embed_dim=16, hidden_dim=128,\n",
    "               meta_epochs=50, inner_steps=1, inner_lr=1e-2, meta_lr=1e-3,\n",
    "               patience=5, min_delta=1e-4):\n",
    "\n",
    "    model = GNN_RE_NER(\n",
    "        input_dim=768+entity_embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_ner_classes=len(ner_label_map),\n",
    "        num_re_classes=len(relation_label_map)\n",
    "    ).to(device)\n",
    "\n",
    "    entity_embedder = nn.Embedding(len(ner_label_map),entity_embed_dim).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=meta_lr)\n",
    "\n",
    "    best, wait = float('inf'), 0\n",
    "    for epoch in range(1,meta_epochs+1):\n",
    "        meta_loss, cnt = 0.0, 0\n",
    "        print(f\"Meta‐Epoch {epoch}/{meta_epochs}\")\n",
    "        for g in graphs:\n",
    "            sg, qg = split_graph(g)\n",
    "            inner_opt = torch.optim.SGD(model.parameters(),lr=inner_lr)\n",
    "            with higher.innerloop_ctx(model, inner_opt, copy_initial_weights=True) as (fmodel, diffopt):\n",
    "                loss_s = compute_loss(fmodel, entity_embedder, sg)\n",
    "                for _ in range(inner_steps):\n",
    "                    diffopt.step(loss_s)\n",
    "                loss_q = compute_loss(fmodel, entity_embedder, qg)\n",
    "                meta_loss += loss_q; cnt+=1\n",
    "        if cnt>0: meta_loss /= cnt\n",
    "\n",
    "        opt.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        opt.step()\n",
    "        print(f\"Avg Query Loss: {meta_loss.item():.4f}\")\n",
    "\n",
    "        if best - meta_loss.item()>min_delta:\n",
    "            best, wait = meta_loss.item(), 0\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait>=patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    return model, entity_embedder\n",
    "\n",
    "# 7. Inference & Annotation\n",
    "def annotate_file(model, entity_embedder, in_path, out_path, thr=0.0):\n",
    "    with open(in_path) as f:\n",
    "        doc = json.load(f)\n",
    "    text = doc.get(\"doc\", doc.get(\"document\",\"\"))\n",
    "    sp = nlp(text)\n",
    "    toks = [t.text for t in sp]\n",
    "    tokens, embs, offs = get_bert_embeddings(text)\n",
    "    feats = align_to_bert(sp, tokens, embs, offs)\n",
    "    edges = [(t.head.i, t.i) for t in sp if t.head.i != t.i]\n",
    "    nf = torch.tensor(feats, dtype=torch.float, device=device)\n",
    "    ner_ids = torch.zeros(len(toks), dtype=torch.long, device=device)\n",
    "    ner_emb = entity_embedder(ner_ids)\n",
    "    X = torch.cat([nf, ner_emb], dim=-1)\n",
    "    E = (\n",
    "        torch.tensor(edges, dtype=torch.long).t().to(device)\n",
    "        if edges\n",
    "        else torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        x1 = model.gcn1(X, E).relu()\n",
    "        x2 = model.gcn2(x1, E).relu()\n",
    "        ner_logits = model.ner_classifier(x2)\n",
    "        pred_ner = ner_logits.argmax(1).cpu().tolist()\n",
    "    ents = group_tokens_into_entities(toks, pred_ner, id2ner)\n",
    "    cand = [\n",
    "        (e1[\"start\"], e2[\"start\"])\n",
    "        for i, e1 in enumerate(ents)\n",
    "        for j, e2 in enumerate(ents)\n",
    "        if i != j\n",
    "    ]\n",
    "    triples = []\n",
    "    if cand:\n",
    "        reps = [torch.cat([x2[h], x2[t]], 0) for h, t in cand]\n",
    "        inp = torch.stack(reps)\n",
    "        with torch.no_grad():\n",
    "            logits = model.re_classifier(inp)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            conf, idxs = probs.max(1)\n",
    "        for i, (h, t) in enumerate(cand):\n",
    "            lab = id2rel[idxs[i].item()]\n",
    "            c = conf[i].item()\n",
    "            if lab != \"NoRelation\" and c >= thr:\n",
    "                triples.append({\n",
    "                    \"head\": ents[[e[\"start\"] for e in ents].index(h)][\"text\"],\n",
    "                    \"tail\": ents[[e[\"start\"] for e in ents].index(t)][\"text\"],\n",
    "                    \"label\": lab,\n",
    "                    \"conf\": c\n",
    "                })\n",
    "\n",
    "    # carry gold labels through so eval can see them\n",
    "    out = {\n",
    "        \"document\":        text,\n",
    "        \"pred_entities\":   ents,\n",
    "        \"pred_triples\":    triples,\n",
    "        \"NER-label_set\":   doc.get(\"NER-label_set\", []),\n",
    "        \"RE_label_set\":    doc.get(\"RE_label_set\",  []),\n",
    "        \"id\":              doc.get(\"id\", \"\")\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\") as fw:\n",
    "        json.dump(out, fw, indent=2)\n",
    "    return out\n",
    "\n",
    "def annotate_all(model, entity_embedder, root_in, root_out, thr=0.0):\n",
    "    for dp, _, files in os.walk(root_in):\n",
    "        rel = os.path.relpath(dp, root_in)\n",
    "        od  = os.path.join(root_out, rel)\n",
    "        os.makedirs(od, exist_ok=True)\n",
    "        for fn in files:\n",
    "            if not fn.endswith(\".json\"):\n",
    "                continue\n",
    "            inp = os.path.join(dp, fn)\n",
    "            out = os.path.join(od, fn)\n",
    "            annotate_file(model, entity_embedder, inp, out, thr)\n",
    "\n",
    "# 8. Evaluation\n",
    "def eval_file(pred_path, gold_path):\n",
    "    pred = json.load(open(pred_path))\n",
    "    gold = json.load(open(gold_path))\n",
    "\n",
    "    # ——— NER type‐level eval ———\n",
    "    gold_ner = set(gold.get(\"NER-label_set\", []))\n",
    "    pred_ner = set(ent[\"label\"] for ent in pred[\"pred_entities\"])\n",
    "    tp = len(gold_ner & pred_ner)\n",
    "    fp = len(pred_ner - gold_ner)\n",
    "    fn = len(gold_ner - pred_ner)\n",
    "    precision_ner = tp / (tp + fp + 1e-8)\n",
    "    recall_ner    = tp / (tp + fn + 1e-8)\n",
    "    f1_ner        = 2 * precision_ner * recall_ner / (precision_ner + recall_ner + 1e-8)\n",
    "\n",
    "    # ——— RE type‐level eval ———\n",
    "    gold_re = set(gold.get(\"RE_label_set\", []))\n",
    "    pred_re = set(tr[\"label\"] for tr in pred[\"pred_triples\"])\n",
    "    tp = len(gold_re & pred_re)\n",
    "    fp = len(pred_re - gold_re)\n",
    "    fn = len(gold_re - pred_re)\n",
    "    precision_re = tp / (tp + fp + 1e-8)\n",
    "    recall_re    = tp / (tp + fn + 1e-8)\n",
    "    f1_re        = 2 * precision_re * recall_re / (precision_re + recall_re + 1e-8)\n",
    "\n",
    "    return precision_ner, recall_ner, f1_ner, precision_re, recall_re, f1_re\n",
    "\n",
    "\n",
    "def eval_all(pred_root, gold_root):\n",
    "    table = []\n",
    "    overall = [0.0]*6\n",
    "    overall_n = 0\n",
    "\n",
    "    for dp, _, files in os.walk(pred_root):\n",
    "        rel = os.path.relpath(dp, pred_root)\n",
    "        sums = [0.0]*6\n",
    "        n = 0\n",
    "        for fn in files:\n",
    "            if not fn.endswith(\".json\"):\n",
    "                continue\n",
    "            pred_path = os.path.join(dp, fn)\n",
    "            gold_path = os.path.join(gold_root, rel, fn)\n",
    "            if not os.path.exists(gold_path):\n",
    "                continue\n",
    "            p1, r1, f1n, p2, r2, f1r = eval_file(pred_path, gold_path)\n",
    "            for i, v in enumerate((p1, r1, f1n, p2, r2, f1r)):\n",
    "                sums[i] += v\n",
    "                overall[i] += v\n",
    "            n += 1\n",
    "            overall_n += 1\n",
    "\n",
    "        if n > 0:\n",
    "            table.append([\n",
    "                os.path.basename(dp),\n",
    "                *(f\"{s/n:.4f}\" for s in sums[:3]),\n",
    "                *(f\"{s/n:.4f}\" for s in sums[3:])\n",
    "            ])\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(\"Meta‐training GNN_RE_NER\")\n",
    "    model, entity_embedder = meta_train(\n",
    "        training_graphs,\n",
    "        ner_label_map,\n",
    "        relation_label_map,\n",
    "        entity_embed_dim=16,\n",
    "        hidden_dim=128,\n",
    "        meta_epochs=50,\n",
    "        inner_steps=1,\n",
    "        inner_lr=1e-2,\n",
    "        meta_lr=1e-3,\n",
    "        patience=5,\n",
    "        min_delta=1e-4\n",
    "    )\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"entity_embedder_state_dict\": entity_embedder.state_dict(),\n",
    "        \"ner_label_map\": ner_label_map,\n",
    "        \"relation_label_map\": relation_label_map\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Saved meta‐trained checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    # 9.3 Annotate test set\n",
    "    TEST_IN  = \"Your_Test_Set_Path\"\n",
    "    TEST_OUT = \"Your_Test_Set_Annotated_Path\"\n",
    "    print(\"Annotating all test documents\")\n",
    "    annotate_all(model, entity_embedder, TEST_IN, TEST_OUT, thr=0.0)\n",
    "\n",
    "    # 9.4 Evaluate\n",
    "    print(\"Evaluation across domains\")\n",
    "    eval_all(TEST_OUT, TEST_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a6d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------+--------+--------+\n",
      "|        Domain        |   P    |   R    |   F1   |\n",
      "+----------------------+--------+--------+--------+\n",
      "| Academic_disciplines | 0.8714 | 0.2450 | 0.3616 |\n",
      "|       Business       | 0.6538 | 0.2637 | 0.3726 |\n",
      "|    Communication     | 0.7000 | 0.2632 | 0.3789 |\n",
      "|       Culture        | 0.7212 | 0.2521 | 0.3611 |\n",
      "|       Economy        | 0.7500 | 0.2571 | 0.3702 |\n",
      "|      Education       | 0.8256 | 0.2767 | 0.4040 |\n",
      "|        Energy        | 0.7500 | 0.2385 | 0.3431 |\n",
      "|     Engineering      | 0.6270 | 0.2257 | 0.3296 |\n",
      "|    Entertainment     | 0.7281 | 0.2687 | 0.3773 |\n",
      "|    Food_and_drink    | 0.7195 | 0.2416 | 0.3507 |\n",
      "|      Geography       | 0.7656 | 0.2629 | 0.3792 |\n",
      "|      Government      | 0.7683 | 0.2608 | 0.3697 |\n",
      "|        Health        | 0.6944 | 0.2508 | 0.3601 |\n",
      "|       History        | 0.7857 | 0.2767 | 0.3945 |\n",
      "|    Human_behavior    | 0.6250 | 0.2425 | 0.3460 |\n",
      "|      Humanities      | 0.7604 | 0.2698 | 0.3881 |\n",
      "|     Information      | 0.7091 | 0.2534 | 0.3628 |\n",
      "|       Internet       | 0.6667 | 0.2426 | 0.3463 |\n",
      "|      Knowledge       | 0.6090 | 0.2352 | 0.3319 |\n",
      "|       Language       | 0.6282 | 0.2606 | 0.3586 |\n",
      "|         Law          | 0.7308 | 0.2475 | 0.3556 |\n",
      "|         Life         | 0.7647 | 0.2549 | 0.3699 |\n",
      "|     Mathematics      | 0.6304 | 0.2638 | 0.3698 |\n",
      "|       Military       | 0.7738 | 0.3018 | 0.4212 |\n",
      "|        Nature        | 0.6842 | 0.2492 | 0.3563 |\n",
      "|        People        | 0.6600 | 0.2539 | 0.3630 |\n",
      "|      Philosophy      | 0.6607 | 0.2373 | 0.3422 |\n",
      "|       Politics       | 0.9434 | 0.3021 | 0.4411 |\n",
      "|       Religion       | 0.7222 | 0.2454 | 0.3538 |\n",
      "|       Science        | 0.6744 | 0.2394 | 0.3484 |\n",
      "|       Society        | 0.7558 | 0.2522 | 0.3641 |\n",
      "|        Sports        | 0.8721 | 0.2679 | 0.3957 |\n",
      "|      Technology      | 0.7759 | 0.3033 | 0.4227 |\n",
      "|       Universe       | 0.6087 | 0.2568 | 0.3603 |\n",
      "+----------------------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "PRED_DIR  = \"Your_Predictions_Path\"\n",
    "GOLD_DIR  = \"Your_Test_Path\"\n",
    "CONF_THR  = 0.018\n",
    "\n",
    "def prf(gold_set, pred_set):\n",
    "    tp = len(gold_set & pred_set)\n",
    "    fp = len(pred_set - gold_set)\n",
    "    fn = len(gold_set - pred_set)\n",
    "    p  = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    r  = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2*p*r/(p+r)     if p + r    > 0 else 0.0\n",
    "    return p, r, f1\n",
    "\n",
    "table = []\n",
    "for pred_root, _, files in os.walk(PRED_DIR):\n",
    "    rel    = os.path.relpath(pred_root, PRED_DIR)\n",
    "    domain = rel if rel != \".\" else \"<root>\"\n",
    "    ner_tp = ner_fp = ner_fn = 0\n",
    "    re_tp  = re_fp  = re_fn  = 0\n",
    "    n_docs = 0\n",
    "    for fn in files:\n",
    "        if not fn.endswith(\".json\"):\n",
    "            continue\n",
    "        pred_path = os.path.join(pred_root, fn)\n",
    "        gold_path = os.path.join(GOLD_DIR,   rel, fn)\n",
    "        if not os.path.exists(gold_path):\n",
    "            continue\n",
    "        pred = json.load(open(pred_path))\n",
    "        gold = json.load(open(gold_path))\n",
    "        gold_ner = set(gold.get(\"NER-label_set\", []))\n",
    "        pred_ner = set(ent[\"label\"] for ent in pred.get(\"pred_entities\", []))\n",
    "        ner_tp += len(gold_ner & pred_ner)\n",
    "        ner_fp += len(pred_ner - gold_ner)\n",
    "        ner_fn += len(gold_ner - pred_ner)\n",
    "        gold_re = set(gold.get(\"RE_label_set\", []))\n",
    "        pred_re = {\n",
    "            tr[\"label\"]\n",
    "            for tr in pred.get(\"pred_triples\", [])\n",
    "            if tr.get(\"conf\", 0.0) >= CONF_THR\n",
    "        }\n",
    "        re_tp  += len(gold_re & pred_re)\n",
    "        re_fp  += len(pred_re - gold_re)\n",
    "        re_fn  += len(gold_re - pred_re)\n",
    "        n_docs += 1\n",
    "    if n_docs == 0:\n",
    "        continue\n",
    "    ner_p  = ner_tp / (ner_tp + ner_fp) if ner_tp + ner_fp > 0 else 0.0\n",
    "    ner_r  = ner_tp / (ner_tp + ner_fn) if ner_tp + ner_fn > 0 else 0.0\n",
    "    ner_f1 = 2*ner_p*ner_r/(ner_p+ner_r) if ner_p+ner_r>0 else 0.0\n",
    "    re_p   = re_tp  / (re_tp  + re_fp)  if re_tp  + re_fp  > 0 else 0.0\n",
    "    re_r   = re_tp  / (re_tp  + re_fn)  if re_tp  + re_fn  > 0 else 0.0\n",
    "    re_f1  = 2*re_p * re_r /(re_p + re_r) if re_p + re_r >0 else 0.0\n",
    "    c_p   = (ner_p + re_p) / 2\n",
    "    c_r   = (ner_r + re_r) / 2\n",
    "    c_f1  = (ner_f1 + re_f1) / 2\n",
    "\n",
    "    table.append([\n",
    "        domain,\n",
    "        f\"{c_p:.4f}\",\n",
    "        f\"{c_r:.4f}\",\n",
    "        f\"{c_f1:.4f}\"\n",
    "    ])\n",
    "\n",
    "print(tabulate(\n",
    "    table,\n",
    "    headers=[\"Domain\", \"P\", \"R\", \"F1\"],\n",
    "    tablefmt=\"pretty\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918baa1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
