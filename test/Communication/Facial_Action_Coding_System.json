{
    "domain": "Communication",
    "document": "The Facial Action Coding System (FACS) is a system to taxonomize human facial movements by their appearance on the face,  based on a system originally developed by a Swedish anatomist named Carl-Herman Hjortsjö. It was later adopted by Paul Ekman and Wallace V. Friesen, and published in 1978. Ekman, Friesen, and Joseph C. Hager published a significant update to FACS in 2002. Movements of individual facial muscles are encoded by the FACS from slight different instant changes in facial appearance. It has proven useful to psychologists and to animators.\nIn 2009, a study was conducted to study spontaneous facial expressions in sighted and blind judo athletes. They discovered that many facial expressions are innate and not visually learned.Using the FACS human coders can manually code nearly any anatomically possible facial expression, deconstructing it into the specific \"action units\" (AU) and their temporal segments that produced the expression. As AUs are independent of any interpretation, they can be used for any higher order decision making process including recognition of basic emotions,  or pre-programmed commands for an ambient intelligent environment. The FACS manual is over 500 pages in length and provides the AUs, as well as Ekman's interpretation of their meanings.The FACS defines AUs,  as contractions or relaxations of one or more muscles. It also defines a number of \"action descriptors\", which differ from AUs in that the authors of the FACS have not specified the muscular basis for the action and have not distinguished specific behaviors as precisely as they have for the AUs.\nFor example, the FACS can be used to distinguish two types of smiles as follows:\nthe insincere and voluntary Pan-Am smile: contraction of zygomatic major alone\nthe sincere and involuntary Duchenne smile: contraction of zygomatic major and inferior part of orbicularis oculi.\nThe FACS is designed to be self-instructional. People can learn the technique from a number of sources including manuals and workshops, and obtain certification through testing.\nAlthough the labeling of expressions currently requires trained experts, researchers have had some success in using computers to automatically identify the FACS codes. One obstacle to automatic FACS code recognition is a shortage of manually coded ground truth data.\nThe use of the FACS has been proposed for use in the analysis of depression, and the measurement of pain in patients unable to express themselves verbally.The original FACS has been modified to analyze facial movements in several non-human primates, namely chimpanzees, rhesus macaques, gibbons and siamangs, and orangutans. More recently, it was developed also for domestic species, including dogs, horses and cats. Similarly to the human FACS, the animal FACS has manuals available online for each species with the respective certification tests.Thus, the FACS can be used to compare facial repertoires across species due to its anatomical basis. A study conducted by Vick and others (2006) suggests that the FACS can be modified by taking differences in underlying morphology into account. Such considerations enable a comparison of the homologous facial movements present in humans and chimpanzees, to show that the facial expressions of both species result from extremely notable appearance changes. The development of FACS tools for different species allows the objective and anatomical study of facial expressions in communicative and emotional contexts. Furthermore, a cross-species analysis of facial expressions can help to answer interesting questions, such as which emotions are uniquely human.\nThe  Emotional Facial Action Coding System (EMFACS) and the  Facial Action Coding System Affect Interpretation Dictionary (FACSAID) consider only emotion-related facial actions. Examples of these are:\nFACS coding is also used extensively in computer animation, in particular for computer facial animation, with facial expressions being expressed as vector graphics of AUs. FACS vectors are used as weights for  blend shapes corresponding to each AU, with the resulting face mesh then being used to render the finished face. Deep learning techniques can be used to determine the FACS vectors from face images obtained during motion capture acting, facial motion capture or other performances.For clarification, the FACS is an index of facial expressions, but does not actually provide any bio-mechanical information about the degree of muscle activation. Though muscle activation is not part of the FACS, the main muscles involved in the facial expression have been added here.Action units (AUs) are the fundamental actions of individual muscles or groups of muscles.\nAction descriptors (ADs) are unitary movements that may involve the actions of several muscle groups (e.g., a forward‐thrusting movement of the jaw). The muscular basis for these actions has not been specified and specific behaviors have not been distinguished as precisely as for the AUs.\nFor the most accurate annotation, the FACS suggests agreement from at least two independent certified FACS encoders.\nIntensities of the FACS are annotated by appending letters A:E (for minimal-maximal intensity) to the action unit number (e.g. AU 1A is the weakest trace of AU 1 and AU 1E is the maximum intensity possible for the individual person).",
    "RE_label_set": [
        "HasPart",
        "OwnerOf",
        "NominatedFor",
        "HasWorksInTheCollection",
        "DifferentFrom",
        "Creator",
        "InfluencedBy",
        "Uses",
        "TypeOfElectrification",
        "UsedBy",
        "HasQuality",
        "AppliesToPeople",
        "LocatedIn",
        "PartOf",
        "Founded",
        "HasEffect",
        "FoundedBy",
        "MemberOf"
    ],
    "NER-label_set": [
        "CARDINAL",
        "DATE",
        "EVENT",
        "FAC",
        "GPE",
        "LANGUAGE",
        "LAW",
        "LOC",
        "MONEY",
        "NORP",
        "ORDINAL",
        "ORG",
        "PERCENT",
        "PERSON",
        "PRODUCT",
        "QUANTITY",
        "TIME",
        "WORK_OF_ART",
        "MISC"
    ],
    "id": "Communication_4"
}